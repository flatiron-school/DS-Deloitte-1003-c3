{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea4724d",
   "metadata": {},
   "source": [
    "# Simple Linear Regression\n",
    "\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28499d28",
   "metadata": {},
   "source": [
    "## Model Assumptions\n",
    "\n",
    "Linear regression models make the following assumptions about the data and the process that generated them.\n",
    "\n",
    "[Here](https://www.statisticssolutions.com/assumptions-of-linear-regression/) is a helpful resource on the assumptions of linear regression.\n",
    "\n",
    "### Linearity\n",
    "\n",
    "**The relationship between the target and predictor is linear.** Check this by drawing a scatter plot of your predictor and your target, and see if there is evidence that the relationship might not follow a straight line.\n",
    "\n",
    "### Independent\n",
    "\n",
    "**The errors are independent**. In other words: Knowing the error for one point doesn't tell you anything about the error for another.\n",
    "\n",
    "### Normality\n",
    "\n",
    "**The errors are normally distributed.** That is, smaller errors are more probable than larger errors, according to the familiar bell curve.\n",
    "\n",
    "**How can I check for this?**\n",
    "- Make a histogram of the residuals\n",
    "- Build a QQ-Plot\n",
    "- Check the Jarque-Bera or Omnibus p-value (from `statsmodels` output)\n",
    "\n",
    "### Homoskedasticity\n",
    "\n",
    "**The errors are homoskedastic.** That is, the errors have the same variance. \n",
    "\n",
    "(The Greek word $\\sigma\\kappa\\epsilon\\delta\\acute{\\alpha}\\nu\\nu\\upsilon\\mu\\iota$ means \"to scatter\".)\n",
    "\n",
    "\n",
    "**How can I check for this?**\n",
    "- Make a scatter plot of the residuals and target values and look to see if they are more or less spread out at different places\n",
    "- Conduct a formal test (e.g. Goldfeld-Quandt)\n",
    "\n",
    "Linear regression models make the following assumptions about the data and the process that generated them.\n",
    "\n",
    "[Here](https://www.statisticssolutions.com/assumptions-of-linear-regression/) is a helpful resource on the assumptions of linear regression. \n",
    "\n",
    "### Violation of the Assumptions\n",
    "\n",
    "No model is perfect, and your assumptions will never hold perfectly. If the violations of assumptions are severe, you can try adjusting the data so the assumptions will hold, such as by... \n",
    "\n",
    "- Transforming your data with a non-linear function (e.g. log)\n",
    "- Only modeling a subset of your data\n",
    "- Dropping outliers\n",
    "\n",
    "These can make it harder to explain or interpret your model, but the trade-off may be worth it. Alternatively, you may be better of just using a different type of model (you will learn many)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7369e3",
   "metadata": {},
   "source": [
    "## Mathematical Proof\n",
    "\n",
    "### Covariance\n",
    "For two variables $X$ and $Y$, each with $n$ values:\n",
    "\n",
    "$$\\sigma_{XY} = \\frac{\\Sigma^n_{i = 1}(x_i - \\mu_x)(y_i - \\mu_y)}{n}$$\n",
    "\n",
    "Note that the value of the covariance is very much a function of the values of X and Y, which can make interpretation difficult. What is wanted is a _standardized_ scale for covariance, hence: _correlation_.\n",
    "\n",
    "### Correlation\n",
    "\n",
    "Pearson Correlation:\n",
    "\n",
    "$$r_P = \\frac{\\Sigma^n_{i = 1}(x_i - \\mu_x)(y_i - \\mu_y)}{\\sqrt{\\Sigma^n_{i = 1}(x_i - \\mu_x)^2\\Sigma^n_{i = 1}(y_i -\\mu_y)^2}}$$\n",
    "\n",
    "Note that we are simply standardizing the covariance by the standard deviations of X and Y (the $n$'s cancel!).\n",
    "\n",
    "We'll always have $-1 \\leq r \\leq 1$. (This was the point of standardizing by the standard deviations of X and Y.)\n",
    "\n",
    "A correlation of -1 means that X and Y are perfectly negatively correlated, and a correlation of 1 means that X and Y are perfectly positively correlated.\n",
    "\n",
    "\n",
    "### Model Specification\n",
    "\n",
    "The solution for a simple regression best-fit line is as follows:\n",
    "\n",
    "$$Y = b + m(X) + \\epsilon$$\n",
    "\n",
    "- slope: \n",
    "\n",
    "$$m = r_P\\frac{\\sigma_y}{\\sigma_x} = \\frac{cov(X, Y)}{var(X)}$$\n",
    "\n",
    "- y-intercept:\n",
    "\n",
    "$$b = \\mu_y - m\\mu_x$$\n",
    "\n",
    "### Proof of Minimizing Error ($\\epsilon$)\n",
    "\n",
    "We demonstrate this by setting the derivative of the loss function, $\\Sigma^n_{i=1}(y_i - (mx_i + b))^2$, equal to 0. **We shall see this calculus trick many times!**\n",
    "\n",
    "For this purpose we consider the loss a function of its optimizing parameters $m$ and $b$. So there are therefore two partial derivatives to consider. (We'll cover this in more depth later in the course.)\n",
    "\n",
    "$$\\frac{\\partial}{\\partial b}[\\sum^n_{i=1}(y_i - mx_i - b)^2] = -2\\sum^n_{i=1}(y_i - mx_i - b)$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial m}[\\sum^n_{i=1}(y_i - mx_i - b)^2] = -2\\sum^n_{i=1}x_i\\sum^n_{i=1}(y_i - mx_i - b)$$\n",
    "\n",
    "- Let's set the first to 0:\n",
    "\n",
    "$$-2\\sum^n_{i=1}(y_i - mx_i - b) = 0$$\n",
    "\n",
    "$$\\sum^n_{i=1}(y_i - mx_i) = \\sum^n_{i=1}b = nb$$\n",
    "\n",
    "**So:** \n",
    "\n",
    "$$b = \\frac{\\sum^n_{i=1}(y_i - mx_i)}{n} = \\mu_y - m\\mu_x$$\n",
    "\n",
    "- Let's set the second to 0:\n",
    "\n",
    "$$-2\\sum^n_{i=1}x_i\\sum^n_{i=1}(y_i - mx_i - b) = 0$$\n",
    "\n",
    "$$\\sum^n_{i=1}(x_iy_i - mx^2_i - bx_i) = 0$$\n",
    "\n",
    "- Plugging in our previous result, we have:\n",
    "\n",
    "$$\\sum^n_{i=1}x_iy_i - (\\frac{1}{n}\\sum^n_{i=1}y_i - \\frac{m}{n}\\sum^n_{i=1}x_i)\\sum^n_{i=1}x_i - m\\sum^n_{i=1}x^2_i = 0$$\n",
    "\n",
    "$$\\sum^n_{i=1}x_iy_i - \\frac{1}{n}\\sum^n_{i=1}x_i\\sum^n_{i=1}y_i + \\frac{m}{n}(\\sum^n_{i=1}x_i)^2 - m\\sum^n_{i=1}x^2_i = 0$$\n",
    "\n",
    "**So:** \n",
    "\n",
    "$$m = \\frac{\\sum^n_{i=1}x_iy_i - \\frac{1}{n}\\sum^n_{i=1}x_i\\sum^n_{i=1}y_i}{\\sum^n_{i=1}x^2_i - \\frac{1}{n}(\\sum^n_{i=1}x_i)^2} = \\frac{n\\times(\\frac{1}{n}\\sum^n_{i=1}x_iy_i - \\frac{1}{n^2}\\sum^n_{i=1}x_i\\sum^n_{i=1}y_i)}{n\\times(\\frac{1}{n}\\sum^n_{i=1}x^2_i - \\mu^2_x)} = \\frac{cov(X, Y)}{var(X)}$$\n",
    "\n",
    "For more on the proof see [here](https://math.stackexchange.com/questions/716826/derivation-of-simple-linear-regression-parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e0fad9",
   "metadata": {},
   "source": [
    "## Coefficient Interpretation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215f21f8",
   "metadata": {},
   "source": [
    "## Model Evaluation \n",
    "\n",
    "### Coefficient of Determination\n",
    "\n",
    "Very often a data scientist will calculate $R^2$, the *coefficient of determination*, as a measure of how well the model fits the data.\n",
    "\n",
    "$R^2$ for a model is ultimately a _relational_ notion. It's a measure of goodness of fit _relative_ to a (bad) baseline model. This bad baseline model is simply the horizontal line $y = \\mu_Y$, for dependent variable $Y$.\n",
    "\n",
    "The actual calculation of $R^2$ is: \n",
    "\n",
    "$$R^2\\equiv 1-\\frac{\\Sigma_i(y_i - \\hat{y}_i)^2}{\\Sigma_i(y_i - \\bar{y})^2}$$\n",
    "\n",
    "$R^2$ is a measure of how much variation in the dependent variable your model explains.\n",
    "\n",
    "### Other Regression Statistics\n",
    "\n",
    "- **F-statistic**: The F-test measures the significance of your model relative to a model in which all coefficients are 0, i.e. relative to a model that says there is no correlation whatever between the predictors and the target.\n",
    "- **Log-Likelihood**: The probability in question is the probability of seeing these data points, *given* the model parameter values. The higher this is, the more our data conform to our model and so the better our fit. AIC and BIC are related to the log-likelihood; we'll talk about those later.\n",
    "- **coef**: These are the betas as calculated by the least-squares regression. We also have p-values and 95%-confidence intervals.\n",
    "- **Omnibus**: This is a test for error normality. The probability is the chance that the errors are normally distributed. \n",
    "- **Durbin-Watson**: This is a test for autocorrelation. We'll return to this topic in a future lecture. \n",
    "- **Jarque-Bera**: This is another test for error normality.\n",
    "- **Cond. No.**: The condition number tests for independence of the predictors. Lower scores are better. When the predictors are *not* independent, we can run into problems of multicollinearity. For more on the condition number, see [here](https://stats.stackexchange.com/questions/168259/how-do-you-interpret-the-condition-number-of-a-correlation-matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc8a794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
