{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfgCfk3_htW5"
   },
   "source": [
    "# Intro to Computer Vision\n",
    "\n",
    "Objectives:\n",
    "\n",
    "  * Gain a firm sense of how machines \"see\" images\n",
    "  * Explore the intuition underlying object detection algorithms\n",
    "  * Learn how to use open-source tools to evaluate how an open-source model performs w.r.t the task of detecting licence plates in images\n",
    "  * Consider the ethics of applying computer vision tools and techniques to a given task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8zXD5ydsJE4"
   },
   "source": [
    "# An overview of how a computer looks at an image, and how to use that for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUPsTMgVsJE5"
   },
   "outputs": [],
   "source": [
    "# Imports!\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import skimage.io as io\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DQWuy5IsJE5"
   },
   "source": [
    "## First, and example of how a computer breaks an image down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "id": "H_U6eSKosJE6",
    "outputId": "eb327e66-eba5-47d4-c1f5-8304dcadb7c2"
   },
   "outputs": [],
   "source": [
    "# Read and Display \n",
    "\n",
    "img = io.imread('http://www.colorwiki.com/images/0/0a/Photodisc.png')\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTWh5p9csJE6"
   },
   "source": [
    "## How the computer looks at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJneEqLFsJE6",
    "outputId": "a8dc3519-166d-4038-eac8-0bd24277d25b"
   },
   "outputs": [],
   "source": [
    "# First, the shape of the photo\n",
    "\n",
    "print(f'There are {img.shape[0]} pixels in the vertical channel')\n",
    "print(f'There are {img.shape[1]} pixels in the horizontal')\n",
    "print(f'There are {img.shape[2]} channels in the \"z-axis\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "id": "AOjfZESasJE6",
    "outputId": "711c5997-a247-45d1-b946-00f45125a35e"
   },
   "outputs": [],
   "source": [
    "# Split into color channels\n",
    "red = img[:, :, 0]\n",
    "green = img[:, :, 1]\n",
    "blue = img[:, :, 2]\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(2,2, figsize=(12,12))\n",
    "\n",
    "\n",
    "# Normal Image\n",
    "cax_00 = axs[0,0].imshow(img)\n",
    "axs[0,0].xaxis.set_major_formatter(plt.NullFormatter())  # kill xlabels\n",
    "axs[0,0].yaxis.set_major_formatter(plt.NullFormatter())  # kill ylabels\n",
    "\n",
    "# Red Channel\n",
    "cax_01 = axs[0,1].imshow(red, cmap='Reds')\n",
    "fig.colorbar(cax_01, ax=axs[0,1])\n",
    "axs[0,1].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "axs[0,1].yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "\n",
    "# Green Channel\n",
    "cax_10 = axs[1,0].imshow(green, cmap='Greens')\n",
    "fig.colorbar(cax_10, ax=axs[1,0])\n",
    "axs[1,0].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "axs[1,0].yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "# Blue Channel\n",
    "cax_11 = axs[1,1].imshow(blue, cmap='Blues')\n",
    "fig.colorbar(cax_11, ax=axs[1,1])\n",
    "axs[1,1].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "axs[1,1].yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJC8OrolsJE6"
   },
   "source": [
    "### Some basic information about the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIsQynmxsJE6",
    "outputId": "e6fcb99c-94da-41c9-dfdb-239f9c10a0cb"
   },
   "outputs": [],
   "source": [
    "# Size of one of the channels\n",
    "\n",
    "red.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZGowUnRlsJE7",
    "outputId": "4a4bea52-b660-4d23-bad3-28d964e5e796"
   },
   "outputs": [],
   "source": [
    "# Example of the data\n",
    "\n",
    "red[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIy3pQ-WsJE7"
   },
   "source": [
    "### A (very) simple filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8ca6eYlsJE7"
   },
   "source": [
    "Using linear algebra, we can transform the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PNrFGi1sJE7"
   },
   "outputs": [],
   "source": [
    "red = red *1.2\n",
    "blue = blue *1.4\n",
    "green = green * 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7kHFJd8sJE7"
   },
   "outputs": [],
   "source": [
    "# re-assign to the proper channel in the original image\n",
    "\n",
    "img[:, :, 0] = red\n",
    "img[:, :, 1] = green\n",
    "img[:, :, 2] = blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "id": "C3dBAKVnsJE7",
    "outputId": "8334d7fa-2c01-49e3-dd73-e482719b4e61"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oh6KD9sLF0Bl"
   },
   "source": [
    "# Using OpenCV for Noise Reduction and Edge Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NL28gg9dOH6"
   },
   "source": [
    "Since edge detection is susceptible to noise in the image, first step is to remove the noise in the image with a Gaussian filter. \n",
    "\n",
    "The smoothened image is then filtered in both horizontal and vertical direction to get first derivative in horizontal direction (Gx) and vertical direction (Gy). From these two images, we can find edge gradient and direction for each pixel.\n",
    "\n",
    "Gradient direction is always perpendicular to edges. It is rounded to one of four angles representing vertical, horizontal and two diagonal directions.\n",
    "\n",
    "After getting gradient magnitude and direction, a full scan of image is done to remove any unwanted pixels which may not constitute the edge. For this, at every pixel, the pixel is evaluated to see if it is a local maximum in its neighborhood in the direction of gradient. Check the image below:\n",
    "\n",
    "![](https://docs.opencv.org/4.x/nms.jpg)\n",
    "\n",
    "Point A is on the edge (in the vertical direction). Gradient direction is normal to the edge. Point B and C are in gradient directions. So point A is checked with point B and C to see if it forms a local maximum. If so, it is considered for next stage, otherwise, it is suppressed (put to zero).\n",
    "\n",
    "In short, the result you get is a binary image with \"thin edges\".\n",
    "\n",
    "A processing stage known as *Hysteresis Thresholding* decides which are all edges are really edges and which are not. For this, we need two threshold values, `minVal` and `maxVal`. Any edges with intensity gradient more than maxVal are sure to be edges and those below minVal are sure to be non-edges, so discarded. Those who lie between these two thresholds are classified edges or non-edges based on their connectivity. If they are connected to \"sure-edge\" pixels, they are considered to be part of edges. Otherwise, they are also discarded. See the image below:\n",
    "\n",
    "![](https://docs.opencv.org/4.x/hysteresis.jpg)\n",
    "\n",
    "The edge A is above the maxVal, so considered as \"sure-edge\". Although edge C is below maxVal, it is connected to edge A, so that also considered as valid edge and we get that full curve. But edge B, although it is above minVal and is in same region as that of edge C, it is not connected to any \"sure-edge\", so that is discarded. So it is very important that we have to select minVal and maxVal accordingly to get the correct result.\n",
    "\n",
    "This stage also removes small pixels noises on the assumption that edges are long lines.\n",
    "\n",
    "So what we finally get is strong edges in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uCvYAgg6FtxV",
    "outputId": "4a7d8317-c16b-44e6-e0b3-c4d67adb7708"
   },
   "outputs": [],
   "source": [
    "!pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7160zHObUlMR",
    "outputId": "c9b24391-a536-4ee2-93ef-16aa53922d6f"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "wget https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_eye_tree_eyeglasses.xml\n",
    "wget https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_alt.xml\n",
    "wget https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_smile.xml\n",
    "wget https://github.com/flatiron-school/DS-Deloitte-1003-c3/main/Lectures/Computer-Vision/images/charlie.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "gs8X8EeBU55n",
    "outputId": "29c13ea8-c38b-4c8c-c418-30eb693a3667"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv.imread('nick.jpeg')\n",
    "img_rgb = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "img_blurred = cv.GaussianBlur(img_gray, (5, 5), 0)\n",
    "edges = cv.Canny(img, 100, 200)\n",
    "\n",
    "plt.subplot(121), plt.imshow(img_rgb)\n",
    "plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(edges)\n",
    "plt.title('Edge Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wH3nTcYnWp_e"
   },
   "source": [
    "# Using OpenCV for Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_uDNF8hfQ_e"
   },
   "source": [
    "Here we will take a look at OpenCV's face detection capabilties. Initially, such an algorithm needs a lot of positive images (images of faces) and negative images (images without faces) to train the classifier. Then we need to extract features from it. For this, Haar features (shown in the image below) are used. They are just like a convolutional kernel. Each feature is a single value obtained by subtracting sum of pixels under the white rectangle from sum of pixels under the black rectangle.\n",
    "\n",
    "![](https://docs.opencv.org/3.4/haar_features.jpg)\n",
    "\n",
    "Now, all possible sizes and locations of each kernel are used to calculate lots of features. For example, a 24x24 window results in over 160,000 features! For each feature calculation, we need to find the sum of the pixels under white and black rectangles. To solve this, they introduced the integral image. However large your image, it reduces the calculations for a given pixel to an operation involving just four pixels. \n",
    "\n",
    "But among all these features we calculated, most of them are irrelevant. For example, consider the image below. The top row shows two good features. The first feature selected seems to focus on the property that the region of the eyes is often darker than the region of the nose and cheeks. The second feature selected relies on the property that the eyes are darker than the bridge of the nose. But the same windows applied to cheeks or any other place is irrelevant. So how do we select the best features out of 160,000+ features? It is achieved by Adaboost. According to the scikit-learn docs, Adaboost is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset, where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\n",
    "\n",
    "![](https://docs.opencv.org/3.4/haar.png)\n",
    "\n",
    "For this, we apply each and every feature on all the training images. For each feature, it finds the best threshold which will classify the faces to positive and negative. Obviously, there will be errors or misclassifications. We select the features with minimum error rate, which means they are the features that most accurately classify the face and non-face images. (The process is not as simple as this. Each image is given an equal weight in the beginning. After each classification, weights of misclassified images are increased. Then the same process is done. New error rates are calculated. Also new weights. The process is continued until the required accuracy or error rate is achieved or the required number of features are found).\n",
    "\n",
    "The final classifier is a weighted sum of these weak classifiers. It is called weak because it alone can't classify the image, but together with others forms a strong classifier. The final setup had around 6000 features.\n",
    "\n",
    "So now you take an image. Take each 24x24 window. Apply 6000 features to it...seems like a lot.\n",
    "\n",
    "In an image, most of the image is non-face region. So it is a better idea to have a simple method to check if a window is not a face region. If it is not, discard it in a single shot, and don't process it again. Instead, focus on regions where there can be a face. This way, we spend more time checking possible face regions.\n",
    "\n",
    "For this, researchers introduced the concept of *Cascade of Classifiers*. Instead of applying all 6000 features on a window, the features are grouped into different stages of classifiers and applied one-by-one. (Normally the first few stages will contain very many fewer features). If a window fails the first stage, discard it. We don't consider the remaining features on it. If it passes, apply the second stage of features and continue the process. The window which passes all stages is a face region. How is that plan!\n",
    "\n",
    "The original researchers' detector had 6000+ features with 38 stages with 1, 10, 25, 25 and 50 features in the first five stages. (The two features in the above image are actually obtained as the best two features from Adaboost). According to the researchers, on average of 10 features out of 6000+ are evaluated per sub-window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "CPjbFct8VCFT",
    "outputId": "3f88eab6-d3d8-4457-ab4c-8b0a81736d1f"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# OpenCV opens images as BRG\n",
    "# but we want it as RGB We'll\n",
    "# also need a grayscale version\n",
    "\n",
    "img_gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "img_rgb = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "\n",
    "classifier = cv.CascadeClassifier('haarcascade_frontalface_alt.xml')\n",
    "\n",
    "detected_objects = classifier.detectMultiScale(img_gray,\n",
    "                                    minSize =(20, 20))\n",
    "\n",
    "amount_found = len(detected_objects)\n",
    "\n",
    "if amount_found != 0:\n",
    "\n",
    "    for (x, y, width, height) in detected_objects:\n",
    "\n",
    "        cv2.rectangle(img_rgb, (x, y),\n",
    "                     (x + height, y + width),\n",
    "                     (0, 255, 0), 5)\n",
    "\n",
    "# Creates the environment of\n",
    "# the picture and shows it\n",
    "\n",
    "plt.subplot(1, 1, 1), plt.xticks([]), plt.yticks([])\n",
    "plt.imshow(img_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooHWlXYwjZH8"
   },
   "source": [
    "### This is ultimately what we're really after:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ClmEWqxeVfr9",
    "outputId": "326032d0-659c-4b3c-e6ac-18fc259502f4"
   },
   "outputs": [],
   "source": [
    "detected_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzMMjJU9ZgGy"
   },
   "source": [
    "#### Let's see how Facebook's open-sourced Detectron2 model performs w.r.t the task of detecting licence plates after being fine-tuned on a dataset of license plate images! 🤯\n",
    "\n",
    "<a href=\"https://colab.research.google.com/drive/1LyI9UMdW6kQqFXdi7yPBoBSmFJ4b0XUq?usp=sharing#offline=true&sandboxMode=true\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoZockVCsJE9"
   },
   "source": [
    "### Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTB01-pFsJE9"
   },
   "source": [
    "The rbg separation was example was adapted from [this stackoverflow](https://stackoverflow.com/questions/39885178/how-can-i-see-the-rgb-channels-of-a-given-image-with-python) article\n",
    "\n",
    "The Canny edge detection material was adapted from [the OpenCV docs](https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html)\n",
    "\n",
    "The Cascade face detection material was also adapted from [the OpenCV docs](https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
